{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbvS9p85YSaf"
      },
      "source": [
        "## **Mounting and Dataset Directory**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M6dFXX0WQBN1"
      },
      "outputs": [],
      "source": [
        "#Drive Mouting\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#Folder Listing\n",
        "!ls \"/content/drive/MyDrive/Patent images\"\n",
        "\n",
        "#Dataset Directory Variable\n",
        "dataset_dir = \"/content/drive/MyDrive/Patent images\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_OSDFfaZyzJ"
      },
      "source": [
        "## **Import Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow opencv-python-headless"
      ],
      "metadata": {
        "id": "vt97BK2vyoYd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SLgV2KuGY1Uv"
      },
      "outputs": [],
      "source": [
        "# Core Libraries\n",
        "import os #used for navigating directories and listing files\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import shutil\n",
        "import io\n",
        "\n",
        "# Deep Learning Framework\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Evaluation Metrics & Visualization\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "#Dataset spliting and training\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Grad-CAM & Image Utilities\n",
        "import cv2\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import imghdr\n",
        "from tqdm import tqdm  # progress bar for checks\n",
        "from PIL import ImageFile\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode, b64encode\n",
        "from IPython.display import display, Javascript, Image\n",
        "from google.colab import output\n",
        "\n",
        "# Suppress TensorFlow warnings for clean outputs\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Reproducibility\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "print(\"All libraries are imported successfully!\")\n",
        "print(\"TensorFlow version:\", tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUR5wH1DbVV2"
      },
      "source": [
        "## **Define Class Mapping and Creating Binary labels**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C5YzP7GCaHHK"
      },
      "outputs": [],
      "source": [
        "# Define mapping: folder name → pest size class\n",
        "mapping = {\n",
        "    'Rat': 'Large',\n",
        "    'Grasshopper': 'Large',\n",
        "    'Beetle': 'Large',\n",
        "    'Wasp': 'Large',\n",
        "    'Dragonfly': 'Large',\n",
        "    'Ant': 'Small',\n",
        "    'Mosquito': 'Small',\n",
        "    'Fly': 'Small',\n",
        "    'Ladybug': 'Small',\n",
        "    'Bee': 'Small',\n",
        "    'Butterfly': 'Small',\n",
        "    'Spider': 'Small'\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vgm-2gf4bsbo"
      },
      "outputs": [],
      "source": [
        "# Create a DataFrame linking every image path to its binary label\n",
        "data = []  # will hold dictionaries with {'filepath': ..., 'label': ...}\n",
        "\n",
        "for species, size_class in mapping.items():\n",
        "    folder_path = os.path.join(dataset_dir, species)\n",
        "\n",
        "    # list all files inside this species folder\n",
        "    for img_file in os.listdir(folder_path):\n",
        "        # build full file path for each image\n",
        "        img_path = os.path.join(folder_path, img_file)\n",
        "\n",
        "        # check for valid image extensions\n",
        "        if img_file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "            data.append({'filepath': img_path, 'label': size_class})\n",
        "\n",
        "# Convert list → pandas DataFrame for easy handling\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Shuffle the DataFrame so data order is random before splitting\n",
        "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Display first few entries to verify structure\n",
        "print(\"Sample entries:\")\n",
        "print(df.head())\n",
        "\n",
        "# Print label distribution to see class balance\n",
        "print(\"\\nLabel distribution:\")\n",
        "print(df['label'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_SjbWQxdG1r"
      },
      "source": [
        "## **Data preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OEDQ-3tlb5uN"
      },
      "outputs": [],
      "source": [
        "\n",
        "corrupt_images = []\n",
        "non_image_files = []\n",
        "\n",
        "for path in tqdm(df['filepath'], desc=\"Checking images\"):\n",
        "    try:\n",
        "        # Check if file is an actual image\n",
        "        if imghdr.what(path) is None:\n",
        "            non_image_files.append(path)\n",
        "            continue\n",
        "\n",
        "        # Try loading the image\n",
        "        img = cv2.imread(path)\n",
        "        if img is None or img.size == 0:\n",
        "            corrupt_images.append(path)\n",
        "\n",
        "    except Exception as e:\n",
        "        corrupt_images.append(path)\n",
        "\n",
        "print(f\"Total corrupt images: {len(corrupt_images)}\")\n",
        "print(f\"Non-image files: {len(non_image_files)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OrKvhbAUddNi"
      },
      "outputs": [],
      "source": [
        "#remove all the non-image enteries from the dataframe\n",
        "df = df[~df['filepath'].isin(non_image_files)]\n",
        "df.reset_index(drop=True, inplace=True)\n",
        "print(f\"Cleaned Dataset:{len(df)}vaid images remaining.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z-XI9pElWWZN"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "extra_corrupt = []\n",
        "\n",
        "for path in tqdm(df['filepath'], desc=\"Double-checking images safely\"):\n",
        "    try:\n",
        "        with Image.open(path) as img:\n",
        "            img.load()  # fully load to confirm it's readable\n",
        "    except Exception:\n",
        "        extra_corrupt.append(path)\n",
        "\n",
        "print(f\"Additional corrupt images found: {len(extra_corrupt)}\")\n",
        "\n",
        "# Remove only truly corrupt files\n",
        "if extra_corrupt:\n",
        "    df = df[~df['filepath'].isin(extra_corrupt)]\n",
        "    df.reset_index(drop=True, inplace=True)\n",
        "    print(f\"Cleaned dataset: {len(df)} valid images remain.\")\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wt4w8OSVZuV7"
      },
      "source": [
        "## **Dataset Split and Image Augmentation Setup**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nuXDWdQWWHko"
      },
      "outputs": [],
      "source": [
        "# Split the data: 70% training, 20% validation, 10% testing\n",
        "train_df, temp_df = train_test_split(df, test_size=0.3, stratify=df['label'], random_state=42)\n",
        "val_df, test_df = train_test_split(temp_df, test_size=0.33, stratify=temp_df['label'], random_state=42)\n",
        "\n",
        "print(f\"Training samples: {len(train_df)}\")\n",
        "print(f\"Validation samples: {len(val_df)}\")\n",
        "print(f\"Testing samples: {len(test_df)}\")\n",
        "\n",
        "# ==========================\n",
        "# ImageDataGenerator Setup\n",
        "# ==========================\n",
        "\n",
        "# Training generator with augmentation to increase robustness\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,              # normalize pixel values\n",
        "    rotation_range=25,           # random rotation\n",
        "    width_shift_range=0.1,       # horizontal shift\n",
        "    height_shift_range=0.1,      # vertical shift\n",
        "    zoom_range=0.2,              # zoom in/out\n",
        "    horizontal_flip=True,        # random horizontal flips\n",
        "    fill_mode='nearest'          # fill missing pixels\n",
        ")\n",
        "\n",
        "# Validation and test sets — only normalization (no augmentation)\n",
        "val_test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# ==========================\n",
        "# Flow generators\n",
        "# ==========================\n",
        "\n",
        "train_generator = train_datagen.flow_from_dataframe(\n",
        "    dataframe=train_df,\n",
        "    x_col='filepath',\n",
        "    y_col='label',\n",
        "    target_size=(224, 224),      # MobileNetV2 input size\n",
        "    batch_size=32,\n",
        "    class_mode='binary',\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "val_generator = val_test_datagen.flow_from_dataframe(\n",
        "    dataframe=val_df,\n",
        "    x_col='filepath',\n",
        "    y_col='label',\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='binary',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "test_generator = val_test_datagen.flow_from_dataframe(\n",
        "    dataframe=test_df,\n",
        "    x_col='filepath',\n",
        "    y_col='label',\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='binary',\n",
        "    shuffle=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9H0Cm_sbZjO"
      },
      "source": [
        "## **Transfer Learning Model (MobilNetV2-based CNN)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Aar6u6jb7dk"
      },
      "source": [
        "Transfer Learning is a technique that reuses knowledge from a model trained on a large generic dataset (e.g., ImageNet) and adapts it to a new, specific task.\n",
        "In this work, we employed MobileNetV2, a convolutional neural network architecture optimized for edge devices, as the feature extractor.\n",
        "The lower convolutional layers of MobileNetV2 were frozen to retain pre-learned spatial and texture representations, while the upper layers were customized to learn discriminative features specific to pest size categories (“Large” vs “Small”).\n",
        "\n",
        "This approach significantly reduces training time, prevents overfitting, and enhances computational efficiency — which is essential for deployment on low-power hardware such as Raspberry Pi or NVIDIA Jetson Nano."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UWzHmP9zbHKT"
      },
      "outputs": [],
      "source": [
        "# Load the base model with pretrained ImageNet weights\n",
        "# exclude the top layers (fully connected classifier part)\n",
        "base_model = MobileNetV2(\n",
        "    weights='imagenet',\n",
        "    include_top=False,\n",
        "    input_shape=(224, 224, 3)\n",
        ")\n",
        "\n",
        "# Freeze base layers so pre-trained weights aren't modified during initial training\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Build custom classification head\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)    # reduce feature maps to single vector per image\n",
        "x = Dense(128, activation='relu')(x)  # learn new relationships specific to pests\n",
        "x = Dropout(0.3)(x)                   # prevent overfitting\n",
        "output = Dense(1, activation='sigmoid')(x)  # binary output: Large (1) or Small (0)\n",
        "\n",
        "# Combine base + custom layers\n",
        "model = Model(inputs=base_model.input, outputs=output)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=1e-4),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Model summary\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9fhx4dzWdPzp"
      },
      "outputs": [],
      "source": [
        "# Unfreeze the last 30 layers for fine-tuning\n",
        "for layer in base_model.layers[-30:]:\n",
        "    layer.trainable = True\n",
        "\n",
        "# Recompile model with a smaller learning rate\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=1e-5),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Callbacks\n",
        "checkpoint = ModelCheckpoint(\n",
        "    \"mobilenetv2_pest_classifier.h5\",\n",
        "    monitor=\"val_accuracy\",\n",
        "    save_best_only=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "early_stop = EarlyStopping(\n",
        "    monitor=\"val_loss\",\n",
        "    patience=5,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WSalMlS7eNNR"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    validation_data=val_generator,\n",
        "    epochs=15,\n",
        "    callbacks=[checkpoint, early_stop],\n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qs5kQbW5eP4x"
      },
      "outputs": [],
      "source": [
        "# Plot accuracy\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Accuracy\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Training vs Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Loss\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Training vs Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing"
      ],
      "metadata": {
        "id": "S4mgkSG_s4NA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load best model (change filename if you used .keras)\n",
        "model = load_model(\"mobilenetv2_pest_classifier.h5\", compile=False)\n",
        "# compile for eval metrics (optional)\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# ---- A.2 Evaluate on test set ----\n",
        "test_loss, test_acc = model.evaluate(test_generator, verbose=1)\n",
        "print(f\"Test accuracy: {test_acc:.4f}, Test loss: {test_loss:.4f}\")\n",
        "\n",
        "# ---- A.3 Predictions and metrics ----\n",
        "# ensure generator is at start\n",
        "test_generator.reset()\n",
        "# predict (sigmoid outputs)\n",
        "preds = model.predict(test_generator, verbose=1)\n",
        "# preds shape = (N,1) or (N,) ; flatten if needed\n",
        "pred_probs = preds.ravel()\n",
        "pred_labels = (pred_probs >= 0.5).astype(int)  # 1 = Large, 0 = Small\n",
        "\n",
        "true_labels = test_generator.classes  # integer encoded by generator (0/1)\n",
        "# Map generator class indices to names (should be {'Large':1,'Small':0} or vice-versa)\n",
        "class_indices = test_generator.class_indices\n",
        "print(\"Class indices mapping:\", class_indices)\n",
        "# build readable class_names in index order\n",
        "inv_map = {v:k for k,v in class_indices.items()}\n",
        "class_names = [inv_map[i] for i in sorted(inv_map.keys())]\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(true_labels, pred_labels, target_names=class_names))\n",
        "\n",
        "# confusion matrix\n",
        "cm = confusion_matrix(true_labels, pred_labels)\n",
        "plt.figure(figsize=(6,5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', xticklabels=class_names, yticklabels=class_names, cmap='Blues')\n",
        "plt.xlabel('Predicted'); plt.ylabel('Actual'); plt.title('Confusion Matrix on Test Set')\n",
        "plt.show()\n",
        "\n",
        "# ---- A.4 Show sample test images with predictions ----\n",
        "import random\n",
        "from tensorflow.keras.preprocessing import image\n",
        "indices = random.sample(range(len(test_generator.filenames)), 9)\n",
        "plt.figure(figsize=(10,10))\n",
        "for i, idx in enumerate(indices):\n",
        "    img_path = test_generator.filepaths[idx]  # full path\n",
        "    img = image.load_img(img_path, target_size=(224,224))\n",
        "    arr = image.img_to_array(img)/255.0\n",
        "    prob = model.predict(arr[None, ...])[0][0]\n",
        "    pred = \"Large\" if prob>=0.5 else \"Small\"\n",
        "    true = inv_map[test_generator.classes[idx]]\n",
        "    ax = plt.subplot(3,3,i+1)\n",
        "    plt.imshow(img)\n",
        "    plt.title(f\"Pred: {pred} ({prob:.2f})\\nTrue: {true}\")\n",
        "    plt.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "49wMRBNcSROY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def take_photo(filename='photo.jpg', quality=0.8):\n",
        "    js = Javascript('''\n",
        "      async function takePhoto(quality) {\n",
        "        const div = document.createElement('div');\n",
        "        const capture = document.createElement('button');\n",
        "        capture.textContent = 'Capture';\n",
        "        div.appendChild(capture);\n",
        "\n",
        "        const video = document.createElement('video');\n",
        "        video.style.display = 'block';\n",
        "        const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "\n",
        "        document.body.appendChild(div);\n",
        "        div.appendChild(video);\n",
        "        video.srcObject = stream;\n",
        "        await video.play();\n",
        "\n",
        "        // Wait for capture\n",
        "        await new Promise((resolve) => capture.onclick = resolve);\n",
        "\n",
        "        const canvas = document.createElement('canvas');\n",
        "        canvas.width = video.videoWidth;\n",
        "        canvas.height = video.videoHeight;\n",
        "        canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "        stream.getVideoTracks()[0].stop();\n",
        "        div.remove();\n",
        "\n",
        "        const data = canvas.toDataURL('image/jpeg', quality);\n",
        "        return data;\n",
        "      }\n",
        "    ''')\n",
        "    display(js)\n",
        "    data = eval_js('takePhoto({})'.format(quality))\n",
        "    binary = b64decode(data.split(',')[1])\n",
        "    with open(filename, 'wb') as f:\n",
        "        f.write(binary)\n",
        "    return filename\n"
      ],
      "metadata": {
        "id": "XMVBQ3e2qazb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Javascript, Image as ColabImage\n",
        "from PIL import Image as PILImage"
      ],
      "metadata": {
        "id": "oH85keiYz910"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    filename = take_photo()  # capture image\n",
        "    print(\"Photo captured successfully!\")\n",
        "\n",
        "    img = PILImage.open(filename).resize((224, 224))  # use PILImage now\n",
        "    img = np.array(img) / 255.0\n",
        "    img = np.expand_dims(img, axis=0)\n",
        "\n",
        "    pred = model.predict(img)[0][0]\n",
        "    label = class_names[int(pred > 0.5)]\n",
        "    confidence = pred if pred > 0.5 else 1 - pred\n",
        "\n",
        "    print(f\"Prediction: {label} ({confidence:.2f} confidence)\")\n",
        "\n",
        "    # Display image in Colab\n",
        "    display(ColabImage(filename=filename))\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"Camera capture failed:\", e)\n"
      ],
      "metadata": {
        "id": "i_zlMK7rzOwH"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}